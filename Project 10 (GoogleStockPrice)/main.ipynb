{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c8c0a01-9c62-4045-8ee6-f627affdb2c0",
   "metadata": {},
   "source": [
    "# PHASE 1 (Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036937d-ed78-4ca9-9424-5cec0d1c34fa",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361b4aa5-d1c9-4ddb-832d-4348a59beb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866338c-e8d4-4712-b151-70d41c94f39a",
   "metadata": {},
   "source": [
    "## Importing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7079088a-2bb2-4a0d-98c4-bd0aa21d20bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv(\"Google_Stock_Price_Train.csv\")\n",
    "training_set = dataset_train.iloc[:, 1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "163378ec-7dd0-49b0-8183-83c8a7ad7b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[325.25],\n",
       "       [331.27],\n",
       "       [329.83],\n",
       "       ...,\n",
       "       [793.7 ],\n",
       "       [783.33],\n",
       "       [782.75]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e55a46b-2732-47b0-88cf-4cb2d09f3f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>325.25</td>\n",
       "      <td>332.83</td>\n",
       "      <td>324.97</td>\n",
       "      <td>663.59</td>\n",
       "      <td>7,380,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>331.27</td>\n",
       "      <td>333.87</td>\n",
       "      <td>329.08</td>\n",
       "      <td>666.45</td>\n",
       "      <td>5,749,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>329.83</td>\n",
       "      <td>330.75</td>\n",
       "      <td>326.89</td>\n",
       "      <td>657.21</td>\n",
       "      <td>6,590,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>328.34</td>\n",
       "      <td>328.77</td>\n",
       "      <td>323.68</td>\n",
       "      <td>648.24</td>\n",
       "      <td>5,405,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2012</td>\n",
       "      <td>322.04</td>\n",
       "      <td>322.29</td>\n",
       "      <td>309.46</td>\n",
       "      <td>620.76</td>\n",
       "      <td>11,688,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>12/23/2016</td>\n",
       "      <td>790.90</td>\n",
       "      <td>792.74</td>\n",
       "      <td>787.28</td>\n",
       "      <td>789.91</td>\n",
       "      <td>623,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>12/27/2016</td>\n",
       "      <td>790.68</td>\n",
       "      <td>797.86</td>\n",
       "      <td>787.66</td>\n",
       "      <td>791.55</td>\n",
       "      <td>789,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>12/28/2016</td>\n",
       "      <td>793.70</td>\n",
       "      <td>794.23</td>\n",
       "      <td>783.20</td>\n",
       "      <td>785.05</td>\n",
       "      <td>1,153,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>12/29/2016</td>\n",
       "      <td>783.33</td>\n",
       "      <td>785.93</td>\n",
       "      <td>778.92</td>\n",
       "      <td>782.79</td>\n",
       "      <td>744,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>12/30/2016</td>\n",
       "      <td>782.75</td>\n",
       "      <td>782.78</td>\n",
       "      <td>770.41</td>\n",
       "      <td>771.82</td>\n",
       "      <td>1,770,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Open    High     Low   Close      Volume\n",
       "0       1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
       "1       1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
       "2       1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
       "3       1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
       "4       1/9/2012  322.04  322.29  309.46  620.76  11,688,800\n",
       "...          ...     ...     ...     ...     ...         ...\n",
       "1253  12/23/2016  790.90  792.74  787.28  789.91     623,400\n",
       "1254  12/27/2016  790.68  797.86  787.66  791.55     789,100\n",
       "1255  12/28/2016  793.70  794.23  783.20  785.05   1,153,800\n",
       "1256  12/29/2016  783.33  785.93  778.92  782.79     744,300\n",
       "1257  12/30/2016  782.75  782.78  770.41  771.82   1,770,000\n",
       "\n",
       "[1258 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d42be-47e8-40f5-87c7-8db22c8a36cf",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37702df7-3d7d-43cc-9aa4-c2fee9854c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommended to use normalization and not standadisation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13dc117a-14d4-4af2-8b97-dfcd887fbda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08581368],\n",
       "       [0.09701243],\n",
       "       [0.09433366],\n",
       "       ...,\n",
       "       [0.95725128],\n",
       "       [0.93796041],\n",
       "       [0.93688146]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e4629-ec0f-49bb-ae4c-890199ef1559",
   "metadata": {},
   "source": [
    "## Creating a DataStructure with 120 timesteps, 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8661c5ee-3dc4-41fc-a5f7-173b5f85d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(120, 1258):\n",
    "    X_train.append(scaled[i-120: i, 0]) # memorize what happened in the prev 60 rows\n",
    "    y_train.append(scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82cc5d7b-487d-45ed-9b8a-e75ea54d8b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08581368, 0.09701243, 0.09433366, ..., 0.0180445 , 0.0180817 ,\n",
       "        0.00710618],\n",
       "       [0.09701243, 0.09433366, 0.09156187, ..., 0.0180817 , 0.00710618,\n",
       "        0.0064923 ],\n",
       "       [0.09433366, 0.09156187, 0.07984225, ..., 0.00710618, 0.0064923 ,\n",
       "        0.00225091],\n",
       "       ...,\n",
       "       [0.78201503, 0.79792023, 0.81851328, ..., 0.95475854, 0.95204256,\n",
       "        0.95163331],\n",
       "       [0.79792023, 0.81851328, 0.82688444, ..., 0.95204256, 0.95163331,\n",
       "        0.95725128],\n",
       "       [0.81851328, 0.82688444, 0.82308952, ..., 0.95163331, 0.95725128,\n",
       "        0.93796041]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "055ad74d-2788-4369-bc87-29f455aeba27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0064923 , 0.00225091, 0.00682714, ..., 0.95725128, 0.93796041,\n",
       "       0.93688146])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee17413-ae03-4041-907a-860bbaa2b046",
   "metadata": {},
   "source": [
    "## Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e70487de-ba51-46b9-bb96-4dad30ba9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b979e56-b5cc-42ab-9d10-7446aaacb67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.08581368],\n",
       "        [0.09701243],\n",
       "        [0.09433366],\n",
       "        ...,\n",
       "        [0.0180445 ],\n",
       "        [0.0180817 ],\n",
       "        [0.00710618]],\n",
       "\n",
       "       [[0.09701243],\n",
       "        [0.09433366],\n",
       "        [0.09156187],\n",
       "        ...,\n",
       "        [0.0180817 ],\n",
       "        [0.00710618],\n",
       "        [0.0064923 ]],\n",
       "\n",
       "       [[0.09433366],\n",
       "        [0.09156187],\n",
       "        [0.07984225],\n",
       "        ...,\n",
       "        [0.00710618],\n",
       "        [0.0064923 ],\n",
       "        [0.00225091]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.78201503],\n",
       "        [0.79792023],\n",
       "        [0.81851328],\n",
       "        ...,\n",
       "        [0.95475854],\n",
       "        [0.95204256],\n",
       "        [0.95163331]],\n",
       "\n",
       "       [[0.79792023],\n",
       "        [0.81851328],\n",
       "        [0.82688444],\n",
       "        ...,\n",
       "        [0.95204256],\n",
       "        [0.95163331],\n",
       "        [0.95725128]],\n",
       "\n",
       "       [[0.81851328],\n",
       "        [0.82688444],\n",
       "        [0.82308952],\n",
       "        ...,\n",
       "        [0.95163331],\n",
       "        [0.95725128],\n",
       "        [0.93796041]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d39845-21b1-464f-b261-a3543b35a97e",
   "metadata": {},
   "source": [
    "# PHASE 2 (Build the RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6fd0d-3f1d-4c56-b549-ac0f009d10e6",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758d9e49-1636-4cf9-ac5b-4e148e14bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e0e30-97fb-4377-bfb8-3c1b717b6569",
   "metadata": {},
   "source": [
    "## Initailizing the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c7463a5-1fd8-4130-90d3-d05e5781d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bd572-0a6c-4b57-9971-26564bfc93b5",
   "metadata": {},
   "source": [
    "## Adding the first LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff5dfdce-bcc6-4984-aec8-d5891aba1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 70, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(rate = 0.2)) # the rate is the percentage of neurons that will be ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c274381-7dda-4a86-a236-8e1d0537bdc9",
   "metadata": {},
   "source": [
    "## Adding a second LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58e5d9e8-2b4a-4398-b6b8-be5875baeeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 70, return_sequences = True))\n",
    "regressor.add(Dropout(rate = 0.2)) # the rate is the percentage of neurons that will be ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc816d-c5c6-4d32-b07e-80f60ef5f816",
   "metadata": {},
   "source": [
    "## Adding a third LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73404551-1668-48ae-a5c5-6eee3d00e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 70, return_sequences = True))\n",
    "regressor.add(Dropout(rate = 0.2)) # the rate is the percentage of neurons that will be ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd43a00-3a9d-47c0-ba13-941c0573f6d3",
   "metadata": {},
   "source": [
    "## Adding a fourth LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba59a394-9355-4344-9a03-60a614bfc650",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 70, return_sequences = False))\n",
    "regressor.add(Dropout(rate = 0.2)) # the rate is the percentage of neurons that will be ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283fb14-d0fc-4a03-b6fe-7a305c91fa7a",
   "metadata": {},
   "source": [
    "## Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3aca0503-9d87-49a7-a832-bf2eb771023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d58f46a-9fc9-4735-870a-17a473a091d8",
   "metadata": {},
   "source": [
    "## Compiling the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "789350f0-30c1-4dac-9732-6fd09f172a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d5818-76da-4193-b409-fb861d44e5cd",
   "metadata": {},
   "source": [
    "## Fitting the RNN to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10248179-2a46-4dff-aabc-5f1bae39d291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "36/36 [==============================] - 10s 136ms/step - loss: 0.0416\n",
      "Epoch 2/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0060\n",
      "Epoch 3/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0048\n",
      "Epoch 4/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0049\n",
      "Epoch 5/130\n",
      "36/36 [==============================] - 4s 123ms/step - loss: 0.0051\n",
      "Epoch 6/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0042\n",
      "Epoch 7/130\n",
      "36/36 [==============================] - 4s 123ms/step - loss: 0.0044\n",
      "Epoch 8/130\n",
      "36/36 [==============================] - 5s 125ms/step - loss: 0.0049\n",
      "Epoch 9/130\n",
      "36/36 [==============================] - 5s 125ms/step - loss: 0.0050\n",
      "Epoch 10/130\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 0.0040\n",
      "Epoch 11/130\n",
      "36/36 [==============================] - 5s 135ms/step - loss: 0.0037\n",
      "Epoch 12/130\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 0.0034\n",
      "Epoch 13/130\n",
      "36/36 [==============================] - 4s 123ms/step - loss: 0.0034\n",
      "Epoch 14/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0034\n",
      "Epoch 15/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0033\n",
      "Epoch 16/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0034\n",
      "Epoch 17/130\n",
      "36/36 [==============================] - 4s 120ms/step - loss: 0.0046\n",
      "Epoch 18/130\n",
      "36/36 [==============================] - 4s 120ms/step - loss: 0.0032\n",
      "Epoch 19/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0032\n",
      "Epoch 20/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0031\n",
      "Epoch 21/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0030\n",
      "Epoch 22/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0030\n",
      "Epoch 23/130\n",
      "36/36 [==============================] - 4s 120ms/step - loss: 0.0029\n",
      "Epoch 24/130\n",
      "36/36 [==============================] - 4s 120ms/step - loss: 0.0028\n",
      "Epoch 25/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0027\n",
      "Epoch 26/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0035\n",
      "Epoch 27/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0033\n",
      "Epoch 28/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0032\n",
      "Epoch 29/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0029\n",
      "Epoch 30/130\n",
      "36/36 [==============================] - 4s 123ms/step - loss: 0.0025\n",
      "Epoch 31/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0025\n",
      "Epoch 32/130\n",
      "36/36 [==============================] - 4s 123ms/step - loss: 0.0027\n",
      "Epoch 33/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0026\n",
      "Epoch 34/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0026\n",
      "Epoch 35/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0027\n",
      "Epoch 36/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0026\n",
      "Epoch 37/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0024\n",
      "Epoch 38/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0023\n",
      "Epoch 39/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0026\n",
      "Epoch 40/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0025\n",
      "Epoch 41/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0025\n",
      "Epoch 42/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0023\n",
      "Epoch 43/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0023\n",
      "Epoch 44/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0021\n",
      "Epoch 45/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0024\n",
      "Epoch 46/130\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.0025\n",
      "Epoch 47/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0026\n",
      "Epoch 48/130\n",
      "36/36 [==============================] - 4s 120ms/step - loss: 0.0020\n",
      "Epoch 49/130\n",
      "36/36 [==============================] - 4s 120ms/step - loss: 0.0020\n",
      "Epoch 50/130\n",
      "36/36 [==============================] - 4s 120ms/step - loss: 0.0022\n",
      "Epoch 51/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0022\n",
      "Epoch 52/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0019\n",
      "Epoch 53/130\n",
      "36/36 [==============================] - 4s 120ms/step - loss: 0.0021\n",
      "Epoch 54/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0019\n",
      "Epoch 55/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0021\n",
      "Epoch 56/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0023\n",
      "Epoch 57/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0020\n",
      "Epoch 58/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0018\n",
      "Epoch 59/130\n",
      "36/36 [==============================] - 5s 125ms/step - loss: 0.0019\n",
      "Epoch 60/130\n",
      "36/36 [==============================] - 4s 123ms/step - loss: 0.0019\n",
      "Epoch 61/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0017\n",
      "Epoch 62/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0017\n",
      "Epoch 63/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0019\n",
      "Epoch 64/130\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0018\n",
      "Epoch 65/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0016\n",
      "Epoch 66/130\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.0018\n",
      "Epoch 67/130\n",
      "36/36 [==============================] - 5s 126ms/step - loss: 0.0016\n",
      "Epoch 68/130\n",
      "36/36 [==============================] - 5s 135ms/step - loss: 0.0018\n",
      "Epoch 69/130\n",
      "36/36 [==============================] - 9s 253ms/step - loss: 0.0015\n",
      "Epoch 70/130\n",
      "36/36 [==============================] - 9s 258ms/step - loss: 0.0016\n",
      "Epoch 71/130\n",
      "36/36 [==============================] - 10s 271ms/step - loss: 0.0017\n",
      "Epoch 72/130\n",
      "36/36 [==============================] - 9s 258ms/step - loss: 0.0015\n",
      "Epoch 73/130\n",
      "36/36 [==============================] - 10s 288ms/step - loss: 0.0017\n",
      "Epoch 74/130\n",
      "36/36 [==============================] - 7s 204ms/step - loss: 0.0016\n",
      "Epoch 75/130\n",
      "36/36 [==============================] - 5s 128ms/step - loss: 0.0016\n",
      "Epoch 76/130\n",
      "36/36 [==============================] - 6s 169ms/step - loss: 0.0015\n",
      "Epoch 77/130\n",
      "36/36 [==============================] - 12s 321ms/step - loss: 0.0014\n",
      "Epoch 78/130\n",
      "36/36 [==============================] - 11s 306ms/step - loss: 0.0014\n",
      "Epoch 79/130\n",
      "36/36 [==============================] - 11s 311ms/step - loss: 0.0016\n",
      "Epoch 80/130\n",
      "36/36 [==============================] - 12s 320ms/step - loss: 0.0016\n",
      "Epoch 81/130\n",
      "36/36 [==============================] - 13s 352ms/step - loss: 0.0013\n",
      "Epoch 82/130\n",
      "36/36 [==============================] - 10s 288ms/step - loss: 0.0014\n",
      "Epoch 83/130\n",
      "36/36 [==============================] - 10s 277ms/step - loss: 0.0016\n",
      "Epoch 84/130\n",
      "36/36 [==============================] - 10s 283ms/step - loss: 0.0014\n",
      "Epoch 85/130\n",
      "36/36 [==============================] - 12s 333ms/step - loss: 0.0013\n",
      "Epoch 86/130\n",
      "36/36 [==============================] - 12s 334ms/step - loss: 0.0013\n",
      "Epoch 87/130\n",
      "36/36 [==============================] - 12s 336ms/step - loss: 0.0014\n",
      "Epoch 88/130\n",
      "36/36 [==============================] - 12s 332ms/step - loss: 0.0014\n",
      "Epoch 89/130\n",
      "36/36 [==============================] - 9s 236ms/step - loss: 0.0015\n",
      "Epoch 90/130\n",
      "36/36 [==============================] - 11s 311ms/step - loss: 0.0014\n",
      "Epoch 91/130\n",
      "36/36 [==============================] - 11s 321ms/step - loss: 0.0013\n",
      "Epoch 92/130\n",
      "36/36 [==============================] - 12s 344ms/step - loss: 0.0014\n",
      "Epoch 93/130\n",
      "36/36 [==============================] - 13s 364ms/step - loss: 0.0013\n",
      "Epoch 94/130\n",
      "36/36 [==============================] - 12s 333ms/step - loss: 0.0013\n",
      "Epoch 95/130\n",
      "36/36 [==============================] - 14s 378ms/step - loss: 0.0012\n",
      "Epoch 96/130\n",
      "36/36 [==============================] - 13s 357ms/step - loss: 0.0013\n",
      "Epoch 97/130\n",
      "36/36 [==============================] - 13s 355ms/step - loss: 0.0013\n",
      "Epoch 98/130\n",
      "36/36 [==============================] - 12s 332ms/step - loss: 0.0012\n",
      "Epoch 99/130\n",
      "36/36 [==============================] - 11s 317ms/step - loss: 0.0012\n",
      "Epoch 100/130\n",
      "36/36 [==============================] - 12s 346ms/step - loss: 0.0011\n",
      "Epoch 101/130\n",
      "36/36 [==============================] - 9s 261ms/step - loss: 0.0012\n",
      "Epoch 102/130\n",
      "36/36 [==============================] - 10s 272ms/step - loss: 0.0011\n",
      "Epoch 103/130\n",
      "36/36 [==============================] - 9s 255ms/step - loss: 0.0011\n",
      "Epoch 104/130\n",
      "36/36 [==============================] - 10s 287ms/step - loss: 0.0012\n",
      "Epoch 105/130\n",
      "36/36 [==============================] - 10s 286ms/step - loss: 0.0012\n",
      "Epoch 106/130\n",
      "36/36 [==============================] - 11s 311ms/step - loss: 0.0012\n",
      "Epoch 107/130\n",
      "36/36 [==============================] - 10s 281ms/step - loss: 0.0011\n",
      "Epoch 108/130\n",
      "36/36 [==============================] - 11s 313ms/step - loss: 0.0013\n",
      "Epoch 109/130\n",
      "36/36 [==============================] - 12s 324ms/step - loss: 0.0011\n",
      "Epoch 110/130\n",
      "36/36 [==============================] - 10s 281ms/step - loss: 0.0012\n",
      "Epoch 111/130\n",
      "36/36 [==============================] - 11s 306ms/step - loss: 0.0011\n",
      "Epoch 112/130\n",
      "36/36 [==============================] - 11s 292ms/step - loss: 0.0011\n",
      "Epoch 113/130\n",
      "36/36 [==============================] - 11s 297ms/step - loss: 0.0011\n",
      "Epoch 114/130\n",
      "36/36 [==============================] - 11s 309ms/step - loss: 0.0014\n",
      "Epoch 115/130\n",
      "36/36 [==============================] - 12s 331ms/step - loss: 0.0011\n",
      "Epoch 116/130\n",
      "36/36 [==============================] - 9s 257ms/step - loss: 0.0011\n",
      "Epoch 117/130\n",
      "36/36 [==============================] - 9s 245ms/step - loss: 0.0012\n",
      "Epoch 118/130\n",
      "36/36 [==============================] - 11s 295ms/step - loss: 0.0011\n",
      "Epoch 119/130\n",
      "36/36 [==============================] - 12s 328ms/step - loss: 0.0011\n",
      "Epoch 120/130\n",
      "36/36 [==============================] - 11s 298ms/step - loss: 0.0012\n",
      "Epoch 121/130\n",
      "36/36 [==============================] - 12s 324ms/step - loss: 0.0013\n",
      "Epoch 122/130\n",
      "31/36 [========================>.....] - ETA: 1s - loss: 0.0010"
     ]
    }
   ],
   "source": [
    "regressor.fit(X_train, y_train, epochs = 130, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f5a15-ec35-4548-be72-2c6eb103afea",
   "metadata": {},
   "source": [
    "# PHASE 3 (Making Predictions & Visualizing results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df3abf-6709-427b-b984-8d7ff647a916",
   "metadata": {},
   "source": [
    "## Get the real stock price of 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e4f48-b234-40c2-b629-0201023f434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv(\"Google_Stock_Price_Test.csv\")\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "real_stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46135c0-2061-4268-91cd-fdce020fee50",
   "metadata": {},
   "source": [
    "## Getting the predicted stock price of 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99870b4a-b387-481c-b894-d9a316cbc1d9",
   "metadata": {},
   "source": [
    "### Preprocessing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10e825-95e6-454b-b0b5-a545c184d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 120:].values\n",
    "inputs = inputs.reshape(-1, 1)\n",
    "inputs = sc.transform(inputs)\n",
    "X_test = []\n",
    "for i in range(120, 140):\n",
    "    X_test.append(inputs[i-120: i, 0]) # memorize what happened in the prev 60 rows\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c233a-1979-42fa-94b9-3600fc48a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf1fa8-f6ec-4d85-8ca5-c160b7182f58",
   "metadata": {},
   "source": [
    "### Visualizing the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21697ce0-b767-47dd-91c6-bc0f32bf9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(real_stock_price, color='green', label = 'Real Price')\n",
    "plt.plot(predicted_stock_price, color='red', label = 'Predicted Price')\n",
    "plt.title(\"Stock Price Predictions\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
